\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{bm}
\usepackage{float}
\usepackage{subfig}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{array}
\usepackage[export]{adjustbox}
\usepackage{lineno,hyperref}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor}
\usepackage{caption}
%\usepackage{subcaption}
\usepackage{graphicx}
\modulolinenumbers[5]
\usepackage[margin=1.5in]{geometry}
\journal{Journal of \LaTeX\ Templates}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%		\fontfamily{qtm}\selectfont
\begin{frontmatter}

\title{Accelerated Magnetic Resonance Imaging by Adversarial Neural Network}

%% Group authors per affiliation:
\author{Ohad Shitrit, Tammy Riklin Raviv}
\address{$^1$Department of Electrical Engineering, 
	     $^2$The Zlotowski Center for Neuroscience
	     Ben-Gurion University of the Negev, Israel}

\begin{abstract}
A main challenge in Magnetic Resonance Imaging (MRI) for clinical applications is speeding up scan time. Beyond the improvement of patient experience and the reduction of operational costs, faster scans are essential for time-sensitive imaging, where target movement is unavoidable, yet must be significantly lessened, e.g., fetal MRI, cardiac cine, and lungs imaging. Moreover, short scan time can enhance temporal resolution in dynamic scans, such as functional MRI or dynamic contrast enhanced MRI. Current imaging methods facilitate MRI acquisition at the price of lower spatial resolution and costly hardware solutions.

We introduce a practical, software-only framework, based on deep learning, for accelerating MRI acquisition, while maintaining anatomically meaningful imaging. This is accomplished by partial MRI sampling, while using an adversarial neural network to directly estimate the missing k-space samples. The inter-play between the generator and the discriminator networks enables the introduction of an adversarial cost in addition to a fidelity loss used for optimizing the peak signal-to-noise ratio (PSNR). Promising image reconstruction results are obtained for 3T and 1.5T brain MRI, from large publicly available dataset, where only 40\%, 25\% and 16.6\% of the raw samples of each scan are used. To assess the clinical usability of the reconstructed images we also performed tissue segmentation and compared the results to those obtained by using the original fully-sampled images.
Segmentation compatibility, measured in terms of Dice scores and Hausdorff distances, demonstrate the quality of the proposed MRI reconstruction with respect to other methods, including the widely-used Compressed Sensing.

\end{abstract}

\begin{keyword}
\text Accelerated MRI \sep Adversarial Loss \sep Deep Neural Network \sep K-space
%\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}
Magnetic Resonance Imaging (MRI) is a non-ionizing imaging modality, and is therefore widely used in diagnostic medicine and biomedical research. The physical principles of MRI are based on a strong magnetic field and pulses of radio frequency (RF) electromagnetic radiation. 
Images are produced when hydrogen atoms, which are prevalent in living organisms, emit the absorbed RF energy that is then received by antennas in close proximity to the anatomy being examined. Spatial localization of the detected MRI signals is obtained by varying the magnetic field gradients. The discretized RF output is presented in a Fourier space
(called K-space), where the x-axis is refers to the frequency and the y-axis to the phase. An inverse fast Fourier transform (IFFT) of the K-space is then used for generating anatomically meaningful MRI scans. Figure \ref{fig:pair_of_k_space_mri} presents K-space traversal patterns used in conventional imaging. Each row of the k-space is acquired after one RF excitation pulse. The number of rows multiplied by the number of slices (z-axis) determines the total scan time.

The duration of standard single structural MRI acquisition is approximately 5 minutes. Usually, several scans of different modalities or a sequence of scans are acquired such that the overall scan time is much longer. Lengthy imaging process reduces patient comfort and is more vulnerable to motion artifacts. In cases where motion is inevitable, e.g., fetal MRI, cardiac cine, and lungs imaging, scan time must be significantly shortened, otherwise the produced images might be useless. Moreover, in dynamic MRI sequences, acquisition must be brief such that the temporal resolution of the sequence would allow capturing significant temporal changes, e.g., instantaneous increment of the contrast-enhanced material concentration in DCE-MRI or differences in hemodynamic response expressed in fMRI \cite{moeller2010multiband}.

A straight forward reduction of the scan time can be obtained by sampling fewer slices, thus reducing the spatial resolution in the z-axis. Spatial distances between adjacent slices of fetal MRI or fMRI, for example, are often as high as 0.5 centimeters. Therefore, a significant portion of the potential input is not conveyed through imaging. On the other hand, under-sampling in the x-y domain leads to aliasing, as predicted by the Nyquist sampling theorem.

Numerous research groups as well as leading MRI scanner manufacturers make significant efforts to accelerate the MRI acquisition process. Hardware solutions allow parallel imaging by using multiple coils \cite{roemer1990nmr} to sample k-space data. There exist two major approaches~\cite{Deshmane2012} that are currently implemented in commercial MRI machines. Both reconstruct an image from the under-sampled k-space data provided by each of the coils. The sensitivity encoder (SENSE) transforms the partial k-spaces into images, then merges the resulting aliased images into one coherent image~\cite{pruessmann1999sense}. 
The GeneRalized Autocalibrating Partial Parallel Acquisition (GRAPPA) techniques~\cite{griswold2002generalized} operate on signal data within the complex frequency domain before the IFFT.

The compressed sensing (CS) technique~\cite{donoho2006compressed} allows efficient acquisition and reconstruction of a signal with fewer samples than the Nyquist-Shannon sampling theorem requires, if the signal has sparse representation in a known transform domain. Using CS for MRI reconstruction by sampling a small subset of the k-space grid had been proposed in~\cite{lustig2007sparse}. The underlying assumption is that the undersampling is random, such that the zero-filled Fourier reconstruction exhibits incoherent artifacts that behave similarly to additive random noise. 
The smooth reconstruction obtained by the CS, may result in a loss of noise-like texture that can be anatomically meaningful.
 
%{\color{red}This, however, makes the algorithm less sensitive to fine details and noise like texture, results in smooth reconstruction with loss of diagnostically information, as we will see in section \ref{experiments_section}.}
%This, however, would require specified pulse programming.
Recently different data driven, machine learning techniques were used for MRI acceleration. 
Ravishankar and Bresler~\cite{ravishankar2011mr} proposed a dictionary-based learning method, to exploit the sparsity of overlapping image patches emphasizing local structure, to reconstruct MRI from highly under-sampled k-space data. In~\cite{caballero2014dictionary}, this concept was extended using spatio-temporal patches for dynamic MRI reconstruction.
Compressed manifold learning based on Laplacian eigenmaps, has been applied in~\cite{usman2014compressive} for respiratory motion estimation and in~\cite{bhatia2015fast} for cardiac MRI reconstruction.

Deep neural networks (DNN) allow to implicitly learn prior information from large datasets. This has been exploited in recent works for MRI reconstruction from sub-sampled data. A convolutional neural network (CNN) was introduced in~\cite{wang2016accelerating} to learn the mapping between zero-filled and fully-sampled MRI.
In~\cite{Oktay2016}, MRI acceleration by super-resolution (SR) concepts was introduced. One of the key ideas is a siamese CNN structure that allows to receive multiple inputs acquired from different viewing planes. Improved low-resolution (LR) high-resolution (HR) mapping was obtained by residual architecture.
A multi-scale residual network, also known as U-net~\cite{ronneberger2015u}, was used in~\cite{lee2017deep} to address the CS optimization, formulated as a residual regression problem.
Hayun at el.~\cite{hyun2017deep}, applied U-net to perform anti-aliasing of MRI generated from sub-sampled k-space. The U-net output image is than enhanced by integrating its k-space domain with the originally acquired raw samples. U-net CNN has been also used in~\cite{sandinodeep} for dynamic MRI reconstruction.
The spatio-temporal characteristics of sequential MRI data was recently exploited using recurrent neural network (RNN) by~\cite{qin2017convolutional}. All these works address the reconstruction problem in the image domain rather than the k-space domain.

The proposed framework utilizes recent advances in deep learning, while addressing MRI reconstruction directly from the k-space. Specifically, we use generative adversarial networks (GAN) \cite{goodfellow2014generative,radford2015unsupervised,pathak2016context}.
GANs are based on the inter-play between two networks: a generator and a discriminator. The generator is capable of learning the distribution over a data-base, and sample realizations of this distribution. 
The discriminator is trained to distinguish between `generated' samples and real ones. This powerful combination has been used for MRI acceleration in~\cite{yu2017deep,mardani2017deep}, and also for generating Computed Tomography (CT)-like images from MRIs~\cite{nie2016medical}.
In contrast, here, the generator is used for reconstruction of the entire k-space grid from under-sampled data. Its loss is a combination of an adversarial loss, calculated on the image domain, based on the discriminator output, and a fidelity loss with respect to the fully sampled k-space. Unlike~\cite{hyun2017deep}, the entire process, including the IFFT, is carried out and optimized in an end-to-end manner by the proposed GAN architecture. As the MRI data is complex, both real and imaginary components are incorporated in both the generator and the discriminator. 

We compare the proposed method to reconstruction results obtained by using a conventional compressed sensing method CS-MRI \cite{lustig2007sparse} and Zero-filling. In addition, we optimize two networks: 1. CNN-L2 - a k-space generator (with same architecture as the proposed method) trained with only L2 loss. 2. IM-CNN-L2 - a network which performs on the image-space and optimized to remove the artifacts caused by under-sampling and zero padding.
To assess the clinical usability of the reconstructed images we perform an extended validation, composed of anatomy and quantitative measurements. In addition to the common used PSNR and Structure Similarity Index (SSIM) we perform tissue segmentation and compared the results to those obtained by using the original fully-sampled images.
Segmentation compatibility, measured in terms of Dice scores and Hausdorff distances, demonstrate the quality of the proposed MRI reconstruction with respect to other methods. The proposed method outperforms the other in 3T and 1.5T brain MRI reconstruction, using a large publicly available dataset, where only 40\%, 25\% and 16.6\% of the raw samples of each scan are used. Moreover, compared to the widely used CS-MRI, the proposed method is faster in a two orders of magnitude. This makes it possible to use it in a real time manner.

The paper is organized as follows. Section \ref{method_section} presents some theoretical foundation and our method. Section \ref{experiments_section} describes the experimental results. Conclusions and future directions are describes in section \ref{conclusions_section}.

%--------------------- METHOD --------------------%
\section{Method}\label{method_section}

\subsection{K-space}
Let $\bm{u}$ denote the desired signal, a 2D MR image, obtained by the IFFT of the complex k-space signal $s_{0}$. Let $M_{F}$ denote a full sampling mask such that the reconstructed MR image is: 
\begin{equation}
\bm{u}=F^{H}M_{F}\odot s_{0}
\end{equation} 
where $H$ is the Hermitian transpose operation, $\odot$ denotes element-wise multiplication, and $F^{H}$ is an orthonormal 2D IFFT operator, such that $F^{H}F=I$. 
While sampling part of the k-space, using $M_{p}$ as a sampling mask, the reconstructed MR image suffers from artifacts and aliasing. An example artifacts caused by under-sampling and zero padding in the phase axis using Gaussian mask (40\%) is shown in Figure \textbf{\ref{fig:pair_of_k_space_mri}}.

\begin{figure}[H]
	\centering
	\begin{tabular}{cccc}
		\subfloat[\textcolor{black}{\scriptsize{}raw k-space}]{\centering{}\includegraphics[width=2.5cm,height=2.5cm]{include/grp1/example_mask_kspace_with_arrows}
			
		} & \subfloat[\textcolor{black}{\scriptsize{}fully sampled MRI}]{\centering{}\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/example_original.png}
		
		} & \subfloat[\textcolor{black}{\scriptsize{}sampling mask}]{\centering{}\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/example_samplingMask4.png}
	
		} & \subfloat[\textcolor{black}{\scriptsize{}Zero-filled}]{\centering{}\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/example_zeroPadding.png}
}\tabularnewline
\end{tabular}\caption{\textcolor{black}{\footnotesize{}{}Under-sampling artifacts: the
	arrows illustrate the sampling methodology}}
\label{fig:pair_of_k_space_mri} 
\end{figure}


\subsection{Objective}

Let $s_{p}=M_{p}\odot s_{0}$ denote the under-sampled k-space. Given a sampling mask and a model $f$, defined by the set of parameters $\Theta$, our goal is to estimate the missing k-space samples such that: 
\begin{equation}
\Theta=\arg\min_{\Theta}L(F^{H}f\left(s_{p};\Theta\right),\boldsymbol{u})
\end{equation}
where $L\left(\cdot\right)$ is the loss function. While choosing the loss to be L2 norm is reasonable for natural images, for the k-space, which has different spatial features, this may not be enough. As mentioned in \cite{pathak2016context}, L2 minimization provides a blurry solution, results in a loss of fine details. 
Averaging the high frequency details in the k-space domain results in very poor reconstruction. In order to address this problem, we used the adversarial loss, based on GAN.

We trained our model using the adversarial strategy, as described in \cite{goodfellow2014generative,radford2015unsupervised}. 
This method is based on a generator $G$, which takes noise \textbf{$z$ }with uniform distribution \textbf{$p_{u}\left(z\right)$} as input and generates samples from the data distribution. A discriminator $D$ is trained to distinguish between ``real'' examples from the data and generated (``fake'') examples from $G.$ During the training process, we optimize $G$ to maximize the discriminator's probability of error. Simultaneously, $D$ is getting better and provides more
accurate predictions.

Let $s_{0}$ denote a ``real'' k-space sample from the distribution $p_{r}\left(s_{0}\right)$. The following optimization process can be described by two-players min-max game:

\begin{equation}
\min_{G}\max_{D}\mathbb{E_{\mathrm{s_{0}\sim p_{r}\left(s_{0}\right)}}\mathrm{\log\left[D\left(x\right)\right]}}+\mathbb{E_{\mathrm{z\sim p_{u}\left(z\right)}}\mathrm{\log\left[1-D\left(G(z)\right)\right]}}
\end{equation}

In equilibrium, the generator $G$ is able to generate samples that look like the real data. In our case, $G$ estimates the missing k-space samples from a linear combination of the sampled data and a uniform noise with distribution $p_{u}\left(z\right)$. An L2 fidelity constraint is added to the adversarial loss of the generator, as follows:

\begin{equation}
\begin{aligned}L_{G}=\alpha\cdot E_{\mathrm{z\sim p_{u}\left(z\right)}}\mathrm{\log\left[1-D\left(F^{-1}\left(\hat{s}_{0}\right)\right)\right]}+\\
\beta\cdot||\left(1-M_{p}\right)\odot\left(\hat{s}_{0}-s_{0}\right)||_{2}^{2}
\end{aligned}
\end{equation}
where $\hat{s}_{0}$ is the estimated k-space and $\alpha=1,\,\beta=1$ are hyperparameters tuned by a cross-validation process. The discriminator's input is the reconstructed MR image,\textbf{ }i.e.,\textbf{ }after IFFT. By that, we are integrating the reconstruction phase and optimize it in an end-to-end manner.
In this work, we use the Wasserstein distance as the GAN loss function~\cite{arjovsky2017wasserstein}, which helps us to improve the training stability and provides better reconstruction compared to the classic GAN used in~\cite{shitrit2017accelerated}.

\subsection{Network Architecture}

The generator input is a two-channel image representing the real and the imaginary parts of the partially sampled k-space image, $s_{p}$. Each missing sample is initialized by uniform i.i.d. noise. The pixel $\left(i,j\right)$ in the generator input image is: 
\begin{equation}
G_{in}\left(i,j\right)=s_{p_{i,j}}+\left(1-M_{p}\right)_{i,j}z_{i,j}
\end{equation}

Due to the combination of the adversarial and the fidelity loss, $G$ produces reasonable k-space samples from a given samples and noise distribution $p_{u}\left(z\right)$. In order to use the sampled data, $s_{p}$, and estimate only the missing samples we used a residual network \cite{he2016deep} as used in \cite{Oktay2016}, such that:
\begin{equation}
\hat{s}_{0}=s_{p}+\left(1-M_{p}\right)\odot G_{out}
\end{equation}
where $G_{out}$ is the generator output. Figure \ref{fig:system} describes our framework:

%\begin{figure}[H]
%	\centering{}\includegraphics[width=7.8cm,height=4cm]{include/grp1/chart}\caption{\label{fig:system} \textcolor{black}{\footnotesize{}{}Framework
%			architecture: $G$ and $D$ are the generator and discriminator networks,
%			respectively. $F$ is a $2D$ IFFT operator.}}
%\end{figure}

\begin{figure}[H]
	\centering{}\includegraphics[width=12.0cm,height=4cm]{include/grp2/system_chart}\caption{\label{fig:system} \textcolor{black}{\footnotesize{}{}Framework
			architecture: $G$ and $D$ are the generator and discriminator networks,
			respectively.}}
\end{figure}

A common architecture is used for the discriminator, composed of convolutional layers, batch normalization, and leaky-ReLU as suggested in \cite{radford2015unsupervised}. For the generator, we compose a dedicated architecture based on multi-channel input for representing the real and imaginary components. Both architectures are shown in Figure \ref{fig:architectures}.\textbf{ }The training methodology is doing $k_{d}$ discriminator update steps for each generator single step.

\begin{figure}[H]
	\centering{}%
	\begin{tabular}{|c|}
		\hline 
		\subfloat[\textcolor{black}{\footnotesize{}Generator}]{\centering{}\includegraphics[width=11cm,height=2cm]{include/grp2/generator_arch} 
			
		}\tabularnewline
		\hline 
		\hline 

		\subfloat[\textcolor{black}{\footnotesize{}Discriminator}]{\centering{}\includegraphics[width=11cm,height=2cm]{include/grp2/discrimonator_arch} 
%		\subfloat[\textcolor{black}{\footnotesize{}Discriminator}]{\centering{}\includegraphics[width=9cm,height=2cm]{include/grp1/arch_discriminator} 
			
		}\tabularnewline
		\hline 
	\end{tabular}\caption{\textcolor{black}{\footnotesize{}{}Networks architecture. Both generator and discriminator input is a two-channel signal, real and imaginary. The number above each layer represents the output channels and all convolutional kernels are $3\times3$.}}
\label{fig:architectures} 
\end{figure}



%--------------------- Experiments --------------------%
\section{Experiments}\label{experiments_section}

The training data consists of 500 $3D$ brain MRI (T1) scans of different patients from the IXI dataset\footnote{\url{http://brain-development.org/ixi-dataset/}}. The data has been acquired by three MR machines, Philips 1.5T, 3T and GE 1.5T. All images padded to resolution of $256\times256$ pixels.
From each $3D$ volume we extract 93 $2D$ saggital slices. We used $37.2k$ $(80\%)$ 2D slices for training and $9.2k$ $(20\%)$ for testing (100 $3D$ volumes).
In order to create k-space images for training, inverse orthonormal
2D FFT is applied to the fully-sampled MR images. 
Further to our work~\cite{shitrit2017accelerated}, which was been focused on 1D Cartesian sampling mask along the phase axis, in this work, we sample the k-space using $2D$ random Gaussian mask. By that, we exploit the sparsity in both phase and frequency dimensions and achieve higher sampling factors of 2.5, 4 and 6 (Figure~\ref{fig:sampling_masks}).
Data augmentation is created by random offsets of the proposed mask and image flipping. This leads us to reconstruction of the MR image from 40\%, 25\% and 16.6\% of the original k-space data.

The generator is composed of $5$ blocks of CONV-BatchNorm-ReLU, with output channels $16,32,64,32,8,$ respectively. The last layer is CONV with two outputs channels (for real and imaginary parts). The discriminator is composed of $4$ blocks of CONV-Pool-BatchNorm-LReLU with output channels $8,16,32,16$ and one fully-connected layer. All CONV layers kernel size is $3\times3$. All weights was initialized by Xavier \cite{glorot2010understanding}. We used RMSprop solver with fixed learning rate of $5\mathrm{e}{-6}$ and set $k_{d}$ to 1.

We compare the proposed method to reconstruction results obtained by using a conventional compressed sensing method CS-MRI \cite{lustig2007sparse} and Zero-filling. In addition, we optimized two networks: 1. CNN-L2 - a k-space generator (with same architecture as the proposed method) trained with only L2 loss. 2. IM-CNN-L2 - a network which performs on the image-space and optimized to remove the artifacts caused by under-sampling and zero padding. 
%We used the U-net \cite{ronneberger2015u} architecture inspired by \cite{lee2017deep}. 
We used the U-net \cite{ronneberger2015u} architecture and the same reconstruction method as presented in~\cite{hyun2017deep}. Specifically, we apply an FFT on the U-net output image to get an estimated k-space. The fully k-space is then recovered by integrating the sampled k-space according to the sampling mask. The same sampling masks was used for all methods.

To assess the clinical usability of the reconstructed images we perform an extended validation, composed of anatomy and quantitative measurements. In addition to the common used PSNR and Structure Similarity Index (SSIM)~\cite{wang2004image} metrics, we perform tissue segmentation and compared the results to those obtained by using the original fully-sampled images. The segmentation compatibility is measured in terms of Dice scores and Hausdorff distances.

\begin{figure}[H]
	\centering
	\begin{tabular}{ccc}
		\subfloat[Factor 2.5 (40\%)]{\includegraphics[width=3cm, height=3cm]{include/grp2/mask2}}
		& 
		\subfloat[Factor 4 (25\%)]{\includegraphics[width=3cm, height=3cm]{include/grp2/mask4}}
		&
		\subfloat[Factor 6 (16.66\%)]{\includegraphics[width=3cm, height=3cm]{include/grp2/mask6}}
		\tabularnewline
		
	\end{tabular}\caption{$2D$ Gaussian sampling masks}
	\label{fig:sampling_masks}
\end{figure}

\subsection{PSNR and SSIM}
PSNR and SSIM are the most commonly used measures for evaluation of image reconstruction quality. PSNR measures the Mean Squared Error (MSE) between the fully-sampled MR image and the reconstructed image. Therefore, this measure is not a sufficient metric for fine details and general objects shape. For example, a model can provides a good reconstruction in the sense of PSNR but with blurry edges and loss of anatomical fine details, results in a poor segmentation, which used for medical diagnostic. However, PSNR is still a good indication for the image quality, especially if an expert should view it.
Quantitative evaluation is presented in~Table~\ref{tbl:PSNR_NO_MASK} and a graphical visualization in ~Figure~\ref{fig:error_psnr_errorbar}. The PSNR calculated on the whole image without masking. Note that the proposed method outperforms the other.

\begin{table}[H]
	\centering{}
	\begin{tabular}{|c||c||c||c|}
		\hline 
		\textbf{PSNR} & \multicolumn{1}{c||}{\multirow{2}{*}{Factor 2.5}} & \multicolumn{1}{c||}{\multirow{2}{*}{Factor 4}} & \multicolumn{1}{c|}{\multirow{2}{*}{Factor 6}} \tabularnewline
		\textbf{Method} & \multicolumn{1}{c||}{} & \multicolumn{1}{c||}{} & \multicolumn{1}{c|}{} \tabularnewline \cline{1-4}
		
		Zero-filled         &32.555~$\pm$~2.294  &26.791~$\pm$~1.856 &16.781~$\pm$~1.737\tabularnewline
		CS-MRI              &39.053~$\pm$~1.750  &33.088~$\pm$~1.929 &26.491~$\pm$~2.587\tabularnewline
		IM-CNN-L2           &39.663~$\pm$~1.934  &35.042~$\pm$~2.042 &28.134~$\pm$~2.181\tabularnewline
		CNN-L2              &39.394~$\pm$~1.985  &33.829~$\pm$~2.034 &31.403~$\pm$~2.040\tabularnewline
		\textbf{Proposed}   &\textbf{40.211~$\pm$~1.902}  &\textbf{35.133~$\pm$~1.870}   &\textbf{32.040~$\pm$~2.110}\tabularnewline
		\hline 
	\end{tabular}\caption{\textcolor{black}{\footnotesize{}{}Error in PSNR, without masking}{\footnotesize{}\label{tbl:PSNR_NO_MASK}}}
\end{table}



\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{include/grp2/error_psnr_errorbar}
\caption{PSNR without masking error-bar}
\end{figure}\label{fig:error_psnr_errorbar}

An additional validation of our model done by calculating the PSNR measure on different brain tissues. We applied an image segmentation algorithm, FAST~\cite{zhang2001segmentation}, on the original fully-sampled MR image and then use it for calculating the masked-PSNR on gray matters, white matters and the Cerebrospinal Fluid (CSF). Results are presented in~Table~\ref{tbl:PSNR_WITH_MASK}.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\scriptsize
		\setlength\tabcolsep{2pt}
		\begin{tabular}{|c||c|c|c||c|c|c||c|c|c|}
			\hline 
			\textbf{PSNR} & \multicolumn{3}{c||}{Factor 2.5} & \multicolumn{3}{c||}{Factor 4} & \multicolumn{3}{c|}{Factor 6}\tabularnewline
			\cline{2-10} 
			\textbf{Method}& White & Gray & CSF & White & Gray & CSF & White & Gray & CSF\tabularnewline
			\hline 
			Zero-filled & 41.49{\tiny $\pm$}3.48 & 36.73{\tiny $\pm$}3.32 & 38.54{\tiny $\pm$}2.45 & 37.27{\tiny $\pm$}2.18 & 34.37{\tiny $\pm$}2.52 & 36.36{\tiny $\pm$}1.87 & 24.88{\tiny $\pm$}1.62 & 24.98{\tiny $\pm$}1.91 & 29.33{\tiny $\pm$}2.08\tabularnewline
			\hline 
			CS-MRI & 44.37{\tiny $\pm$}4.20 & 40.04{\tiny $\pm$}3.84 & 40.74{\tiny $\pm$}3.16 & 41.65{\tiny $\pm$}3.12 & 37.24{\tiny $\pm$}2.99 & 39.11{\tiny $\pm$}2.43 & 35.59{\tiny $\pm$}3.15 & 33.11{\tiny $\pm$}2.91 & 36.64{\tiny $\pm$}2.26\tabularnewline
			\hline
%			IM-CNN-L2 & 42.99{\tiny $\pm$}3.81 & 38.82{\tiny $\pm$}3.69 & 39.92{\tiny $\pm$}3.11 & 40.57{\tiny $\pm$}2.66 & 37.10{\tiny $\pm$}2.90 & 38.47{\tiny $\pm$}2.31 & 34.74{\tiny $\pm$}2.89 & 31.49{\tiny $\pm$}2.43 & 33.71{\tiny $\pm$}1.80\tabularnewline
			IM-CNN-L2  & 45.66{\tiny $\pm$}4.60 & 41.06{\tiny $\pm$}4.27 & 41.51{\tiny $\pm$}3.58 & 43.47{\tiny $\pm$}3.22 & 39.36{\tiny $\pm$}3.19 & 40.16{\tiny $\pm$}2.51 & 37.34{\tiny $\pm$}3.19 & 34.18{\tiny $\pm$}2.85 & 36.61{\tiny $\pm$}2.10\tabularnewline
			\hline  
			CNN-L2 & 45.72{\tiny $\pm$}4.70 & 41.59{\tiny $\pm$}4.16 & 41.68{\tiny $\pm$}3.69 & 43.26{\tiny $\pm$}2.95 & 40.15{\tiny $\pm$}3.15 & 40.24{\tiny $\pm$}2.47 & 41.11{\tiny $\pm$}2.48 & 39.27{\tiny $\pm$}2.65 & 39.10{\tiny $\pm$}1.93\tabularnewline
			\hline 
			\textbf{Proposed} & \textbf{46.20{\tiny $\pm$}4.77} & \textbf{41.94{\tiny $\pm$}4.34} & \textbf{41.61{\tiny $\pm$}2.45} & \textbf{43.85{\tiny $\pm$}3.16} & \textbf{40.11{\tiny $\pm$}3.25} & \textbf{40.49{\tiny $\pm$}2.57} & \textbf{41.91{\tiny $\pm$}2.77} & \textbf{39.01{\tiny $\pm$}2.88} & \textbf{39.36{\tiny $\pm$}2.17}\tabularnewline
			\hline 
		\end{tabular}
	}
	\caption{\textcolor{black}{\footnotesize{}{}Error in PSNR, with masking}{\footnotesize{}\label{tbl:PSNR_WITH_MASK}}}
\end{table}


SSIM is used for measuring the similarity between the original and the reconstructed images.
It quantifies the degradation caused by sampling artifacts based on visible structures in the image. SSIM values for different sampling factors are presented in~Table~\ref{tbl:SSIM_NO_MASK}.

\begin{table}[H]
	\centering{}
	\begin{tabular}{|c||c||c||c|}
		\hline 
		\textbf{SSIM} & \multicolumn{1}{c||}{\multirow{2}{*}{Factor 2.5}} & \multicolumn{1}{c||}{\multirow{2}{*}{Factor 4}} & \multicolumn{1}{c|}{\multirow{2}{*}{Factor 6}} \tabularnewline
		\textbf{Method} & \multicolumn{1}{c||}{} & \multicolumn{1}{c||}{} & \multicolumn{1}{c|}{} \tabularnewline \cline{1-4}
		
		Zero-filled         &0.698~$\pm$~0.0452  &0.590~$\pm$~0.040   &0.255~$\pm$~0.021\tabularnewline
		CS-MRI              &0.865~$\pm$~0.029  &0.757~$\pm$~0.037   &0.617~$\pm$~0.030\tabularnewline
		IM-CNN-L2           &0.884~$\pm$~0.030  &0.796~$\pm$~0.036   &0.606~$\pm$~0.023\tabularnewline
		CNN-L2              &0.885~$\pm$~0.030  &0.749~$\pm$~0.043   &0.682~$\pm$~0.042\tabularnewline
		\textbf{Proposed}   &\textbf{0.917~$\pm$~0.019}  &\textbf{0.818~$\pm$~0.034}   &\textbf{0.726~$\pm$~0.038}\tabularnewline
		\hline 
	\end{tabular}\caption{\textcolor{black}{\footnotesize{}{}SSIM for different sampling factors}{\footnotesize{}\label{tbl:SSIM_NO_MASK}}}
\end{table}


\subsection{Brain Extraction - Skull Stripping}
Brain extraction (Skull stripping) is a an algorithm that delineates the brain boundary (Figure~ \ref{fig:brain_extraction}). It is necessary for almost every brain analysis algorithm. In tissue segmentation for example, skull stripping is a pre-processing step which affects directly on the segments partition. We examine the different reconstruction methods by applying the Brain Extraction Tool (BET) \cite{smith2002fast} on each different MR reconstructed image. Then, we compared the skull stripping to the fully-sampled results using the Modified Hausdorff Distance (MHD)~\cite{dubuisson1994modified}.

Let $C_1,C_2\in R^2$ denotes the brain contours, extract from skull stripping algorithm, for a two different reconstruction methods respectively (in our case the fully-sampled and the tested reconstruction method). MHD measures the distance between the contours such that:

\begin{equation}
\begin{array}{cc}
MHD(C_1,C_2) = \max \left\{\frac{1}{|C_1|} \sum_{\sigma_{1}\in C_1}^{}d(\sigma_1,C_2), ~ \frac{1}{|C_2|} \sum_{\sigma_{2}\in C_2}^{}d(\sigma_2,C_1)\right\} \\
d(\sigma_1,C_2) = \underset{\sigma_{2}\in C_{2}}{\min}||\sigma_1-\sigma_2|| \\
d(\sigma_2,C_1) = \underset{\sigma_{1}\in C_{1}}{\min}||\sigma_2-\sigma_1||
\end{array}
\end{equation}
where $\sigma_1,\sigma_2$ are points on the contours $C_1,C_2$ respectively (Figure~\ref{fig:brain_extraction}.c). MHD values for different sampling ratios are presented in~Table~\ref{tbl:MHD}.

\begin{table}[H]
	\centering{}
	\begin{tabular}{|c||c||c||c|}
		\hline 
		\textbf{MHD} & \multicolumn{1}{c||}{\multirow{2}{*}{Factor 2.5}} & \multicolumn{1}{c||}{\multirow{2}{*}{Factor 4}} & \multicolumn{1}{c|}{\multirow{2}{*}{Factor 6}} \tabularnewline
		\textbf{Method} & \multicolumn{1}{c||}{} & \multicolumn{1}{c||}{} & \multicolumn{1}{c|}{} \tabularnewline \cline{1-4}
				
		Zero-filled         &1.111~$\pm$~0.563  &2.617~$\pm$~1.214   &3.121~$\pm$~1.279\tabularnewline
		CS-MRI              &0.701~$\pm$~0.511  &1.447~$\pm$~1.027   &3.114~$\pm$~1.617\tabularnewline
%		IM-CNN-L2           &0.654~$\pm$~0.414  &0.878~$\pm$~0.548   &2.760~$\pm$~1.748\tabularnewline
		IM-CNN-L2           &0.541~$\pm$~0.388  &0.724~$\pm$~0.394   &1.902~$\pm$~1.437\tabularnewline
		CNN-L2              &0.420~$\pm$~0.270  &0.715~$\pm$~0.561   &1.083~$\pm$~1.052\tabularnewline
		\textbf{Proposed}   &\textbf{0.391~$\pm$~0.250}  &\textbf{0.617~$\pm$~0.306}   &\textbf{1.050~$\pm$~1.033}\tabularnewline
		\hline 
	\end{tabular}\caption{\textcolor{black}{\footnotesize{}{}MHD - brain extraction}{\footnotesize{}\label{tbl:MHD}}}
\end{table}


\begin{figure}[H]
	\centering
	\begin{tabular}{ccc}
	\subfloat[MR image]{\includegraphics[width=3cm, height=3.5cm]{include/grp2/skull_stripping}} 
	& 
	\subfloat[Brain extraction]{\includegraphics[width=3cm, height=3.5cm]{include/grp2/skull_stripping_brain}}
	&
	\subfloat[Hausdorff distance]{\includegraphics[width=3.5cm, height=3.5cm]{include/grp2/hausdorff}}
	\tabularnewline

	\end{tabular}\caption{Example of brain extraction}
	\label{fig:brain_extraction}
\end{figure}


\subsection{Brain Segmentation}
Brain segmentation is the task of partitioning the brain image (MRI in our case), into the following segments: background, white matter, gray matter and CSF. This process is of a great importance for brain MR image analysis, especially for therapy planning and also for clinical research. Sampling artifacts, results in poor signal to noise ratio, blurry edges and loss of anatomical fine details, leads to bad segmentation performance.
Segmentation compatibility demonstrate the quality of the proposed MRI reconstruction with respect to other methods. We create a reference segmentation by extracting the brain (using BET \cite{smith2002fast}) from the original fully-sampled image and perform a brain segmentation algorithm (FAST \cite{zhang2001segmentation}) on it. Then, we did the same process for all the reconstruction methods and compared the segmentation results.

The measure used for segmentations comparison is the Dice score \cite{dice1945measures} between the original and the reconstructed image segments. Dice score is a measure of sets similarity which gets the values between 0 (no similarity) to 1.0 (max similarity). Denotes by $S_{i,j}$ the set of pixels in segment $j$ for reconstruction method $i$, the Dice score is:
\begin{equation}
\begin{array}{c}
DICE(S_{ref,j},S_{i,j}) = \frac{2\cdot|S_{ref,j} \cap S_{i,j}|} {|S_{ref,j}| + |S_{i,j}|}
\end{array}
\end{equation}
Dice values for different sampling factors are presented in~Table~\ref{tbl:DICE}. Our methods achieved the best Dice scores for all brain tissues.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\scriptsize
		\setlength\tabcolsep{2pt}
		\begin{tabular}{|c||c|c|c||c|c|c||c|c|c|}
			\hline 
			\textbf{DICE} & \multicolumn{3}{c||}{Factor 2.5} & \multicolumn{3}{c||}{Factor 4} & \multicolumn{3}{c|}{Factor 6}\tabularnewline
			\cline{2-10} 
			\textbf{Method}& White & Gray & CSF & White & Gray & CSF & White & Gray & CSF\tabularnewline
			\hline 
			Zero-filled & 0.882{\tiny $\pm$}0.08 & 0.826{\tiny $\pm$}0.05 & 0.796{\tiny $\pm$}0.05 & 0.718{\tiny $\pm$}0.10 & 0.644{\tiny $\pm$}0.05 & 0.627{\tiny $\pm$}0.05 & 0.599{\tiny $\pm$}0.12 & 0.466{\tiny $\pm$}0.07 & 0.050{\tiny $\pm$}0.06\tabularnewline
			\hline 
			CS-MRI & 0.942{\tiny $\pm$}0.05 & 0.910{\tiny $\pm$}0.03 & 0.868{\tiny $\pm$}0.03 & 0.871{\tiny $\pm$}0.10 & 0.805{\tiny $\pm$}0.08 & 0.770{\tiny $\pm$}0.05 & 0.708{\tiny $\pm$}0.14 & 0.639{\tiny $\pm$}0.08 & 0.621{\tiny $\pm$}0.07     \tabularnewline
			\hline 

%			IM-CNN-L2 & 0.908{\tiny $\pm$}0.04 & 0.863{\tiny $\pm$}0.03 & 0.839{\tiny $\pm$}0.03 & 0.832{\tiny $\pm$}0.05 & 0.754{\tiny $\pm$}0.03 & 0.754{\tiny $\pm$}0.03 & 0.695{\tiny $\pm$}0.11 & 0.590{\tiny $\pm$}0.06 & 0.560{\tiny $\pm$}0.06     \tabularnewline
%			\hline
			
			IM-CNN-L2  & 0.947{\tiny $\pm$}0.02 & 0.917{\tiny $\pm$}0.02 & 0.887{\tiny $\pm$}0.02 & 0.903{\tiny $\pm$}0.03 & 0.853{\tiny $\pm$}0.02 & 0.828{\tiny $\pm$}0.02 & 0.741{\tiny $\pm$}0.14 & 0.681{\tiny $\pm$}0.08 & 0.674{\tiny $\pm$}0.07     \tabularnewline
			\hline
			
			CNN-L2 & 0.948{\tiny $\pm$}0.02 & 0.919{\tiny $\pm$}0.02 & 0.891{\tiny $\pm$}0.02 & 0.888{\tiny $\pm$}0.06 & 0.836{\tiny $\pm$}0.04 & 0.813{\tiny $\pm$}0.03 & 0.836{\tiny $\pm$}0.09 & 0.770{\tiny $\pm$}0.06 & 0.747{\tiny $\pm$}0.06     \tabularnewline
			\hline
			
			\textbf{Proposed} & \textbf{0.954{\tiny $\pm$}0.01} & \textbf{0.928{\tiny $\pm$}0.02} & \textbf{0.900{\tiny $\pm$}0.002} & \textbf{0.903{\tiny $\pm$}0.06} & \textbf{0.858{\tiny $\pm$}0.04} & \textbf{0.833{\tiny $\pm$}0.03} & \textbf{0.851{\tiny $\pm$}0.09} & \textbf{0.789{\tiny $\pm$}0.06} & \textbf{0.767{\tiny $\pm$}0.06}     \tabularnewline
			\hline 
		\end{tabular}
	}
	
	\caption{\textcolor{black}{\footnotesize{}{}Segmentation Dice score for different sampling ratios}{\footnotesize{}\label{tbl:DICE}}}
	
\end{table}

Visual comparison for sampling factor 6 is shown in Figure~\ref{fig:example_factor_6}. The first and the second rows are the reconstructed images with zoom in of red ROI. The third row is the extracted brain and the last row is the calculated segmentation. Each column represents different reconstruction method. MRIs reconstructed by using the suggested adversarial method have stronger contrast and no significant aliasing or artifacts. In addition the brain extraction and the segmentation are more accurate. Additional examples for factor 4 and 2.5 can be found in Figures~\ref{fig:example_factor_4},~\ref{fig:example_factor_2.5}.

% Examples factor 6
\begin{figure}[H]
	%	\begin{raggedright}
	\begin{raggedleft}
		%		\begin{tabular}{>{\centering}b{0.2cm}lcccc}
		\hspace*{-2cm} \begin{tabular}{cccccc}
			& \multicolumn{1}{c}{\footnotesize Original} & {\footnotesize Zero-filled} & {\footnotesize CS-MRI} & {\footnotesize IM-CNN-L2} & {\footnotesize Proposed}\tabularnewline
			\multirow{1}{0.05cm}[1.8cm]{\begin{turn}{90} {\footnotesize Reconstructed} \end{turn}} &
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_images__50} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_images__zeroPadding_50} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_images__CS_50} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_images__IMCNNL2TUNE_50} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_images__predict_50}
			
			\tabularnewline
			
			\multirow{1}{0.05cm}[1.3cm]{\begin{turn}{90} {\footnotesize Zoom in} \end{turn}} &
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_images__zoom_50} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_images__zeroPadding_zoom_50} & 
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_images__CS_zoom_50} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_images__IMCNNL2TUNE_zoom_50} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_images__predict_zoom_50}
			
			\tabularnewline
			
			\multirow{2}{0.05cm}[1.4cm]{\begin{turn}{90} {\footnotesize Brain} \end{turn}} & 
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_brains__50} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_brains__zeroPadding_50} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_brains__CS_50} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_brains__IMCNNL2TUNE_50} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_brains__predict_50}
			
			\tabularnewline
			
			\multirow{2}{0.05cm}[1.7cm]{\begin{turn}{90} {\footnotesize Segmentation} \end{turn}} &
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_segs__50} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_segs__zeroPadding_50} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_segs__CS_50} & \includegraphics[width=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_segs__IMCNNL2TUNE_50} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor6/022-Guys-0701-T1/022-Guys-0701-T1_segs__predict_50}
			
			
		\end{tabular}
		\par\end{raggedleft}
	\raggedright{}\caption{\textcolor{black}{\footnotesize{}Examples of reconstructed MR images from under-sampled k-space - Factor 6.}}
	\label{fig:example_factor_6} 
\end{figure}

%--------------------- CONCLUSIONS --------------------%
\section{Discussion}\label{conclusions_section}

We proposed a practical, software-only framework, based on deep learning, for accelerating MRI acquisition, while maintaining anatomically meaningful imaging. This is accomplished by partial MRI sampling, while using an adversarial neural network to directly estimate the missing k-space samples from the real k-space distribution. Promising image reconstruction results are obtained for 3T and 1.5T brain MRI, from large publicly available dataset, where only 40\%, 25\% and 16.6\% of the raw samples of each scan are used.

Our method produce clinically acceptable reconstructions.  Compared to the methods being tested, our method achieves the best PSNR and SSIM. We also perform tissue segmentations and measure the performance using the Dice scores, with respect to those obtained by using the original fully-sampled images. Better Dice score for white matter, gray matter, and CSF were achieved for all sampling factors. Moreover, lower Hausdorff distance indicates the quality of the proposed method in terms of general brain shape, especially comparing to CS, as seen in Figures~\ref{fig:example_factor_6},~\ref{fig:example_factor_4},~\ref{fig:example_factor_2.5}.

Additional advantage of the proposed method is the runtime. A few days are required for training the generator. Once the network is optimized, the reconstruction for a single test image is done in less than 10 msec, almost two orders of magnitude faster than the widely used CS. This fact makes it possible to use it in a real time manner.
Future work will concentrate on generation of MRI in the presence of pathologies.

{\color{red}
Do we need to discuss about :
K-SPACE VS IMAGE SPACE ??
}

%--------------------- Acknowledgment --------------------%
\subsection{Acknowledgment}
This study was partially supported by The Israel Science Foundation (1638/16); The Israel Defense Forces (IDF) Medical Corps and Directorate of Defense Research \& Development, Israeli Ministry of Defense (IMOD DDR\&D); The Israeli ministry of science, technology and space (63551).
%This research is partially supported by the Israel Science Foundation (T.R.R. 1638/16 ) and the IDF Medical Corps (T.R.R.)


% Examples factor 4
\begin{figure}[H]
	%	\begin{raggedright}
	\begin{raggedleft}
		%		\begin{tabular}{>{\centering}b{0.2cm}lcccc}
		\hspace*{-2cm} \begin{tabular}{cccccc}
			& \multicolumn{1}{c}{\footnotesize Original} & {\footnotesize Zero-filled} & {\footnotesize CS-MRI} & {\footnotesize IM-CNN-L2} & {\footnotesize Proposed}\tabularnewline
			\multirow{1}{0.05cm}[1.8cm]{\begin{turn}{90} {\footnotesize Reconstructed} \end{turn}} &
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_images__35} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_images__zeroPadding_35} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_images__CS_35} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_images__IMCNNL2TUNE_35} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_images__predict_35}
			
			\tabularnewline
			
			\multirow{1}{0.05cm}[1.3cm]{\begin{turn}{90} {\footnotesize Zoom in} \end{turn}} &
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_images__zoom_35} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_images__zeroPadding_zoom_35} & 
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_images__CS_zoom_35} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_images__IMCNNL2TUNE_zoom_35} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_images__predict_zoom_35}
			
			\tabularnewline
			
			\multirow{2}{0.05cm}[1.4cm]{\begin{turn}{90} {\footnotesize Brain} \end{turn}} & 
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_brains__35} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_brains__zeroPadding_35} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_brains__CS_35} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_brains__IMCNNL2TUNE_35} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_brains__predict_35}
			
			\tabularnewline
			
			\multirow{2}{0.05cm}[1.7cm]{\begin{turn}{90} {\footnotesize Segmentation} \end{turn}} &
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_segs__35} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_segs__zeroPadding_35} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_segs__CS_35} & \includegraphics[width=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_segs__IMCNNL2TUNE_35} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor4/012-HH-1211-T1/012-HH-1211-T1_segs__predict_35}
			
			
		\end{tabular}
		\par\end{raggedleft}
	\raggedright{}\caption{\textcolor{black}{\footnotesize{}Examples of reconstructed MR images from under-sampled k-space - Factor 4.}}
	\label{fig:example_factor_4} 
\end{figure}

% Examples factor 2.5
\begin{figure}[H]
	%	\begin{raggedright}
	\begin{raggedleft}
		%		\begin{tabular}{>{\centering}b{0.2cm}lcccc}
		\hspace*{-2cm} \begin{tabular}{cccccc}
			& \multicolumn{1}{c}{\footnotesize Original} & {\footnotesize Zero-filled} & {\footnotesize CS-MRI} & {\footnotesize IM-CNN-L2} & {\footnotesize Proposed}\tabularnewline
			\multirow{1}{0.05cm}[1.8cm]{\begin{turn}{90} {\footnotesize Reconstructed} \end{turn}} &
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_images__78} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_images__zeroPadding_78} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_images__CS_78} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_images__IMCNNL2TUNE_78} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_images__predict_78}
			
			\tabularnewline
			
			\multirow{1}{0.05cm}[1.3cm]{\begin{turn}{90} {\footnotesize Zoom in} \end{turn}} &
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_images__zoom_78} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_images__zeroPadding_zoom_78} & 
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_images__CS_zoom_78} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_images__IMCNNL2TUNE_zoom_78} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_images__predict_zoom_78}
			
			\tabularnewline
			
			\multirow{2}{0.05cm}[1.4cm]{\begin{turn}{90} {\footnotesize Brain} \end{turn}} & 
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_brains__78} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_brains__zeroPadding_78} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_brains__CS_78} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_brains__IMCNNL2TUNE_78} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_brains__predict_78}
			
			\tabularnewline
			
			\multirow{2}{0.05cm}[1.7cm]{\begin{turn}{90} {\footnotesize Segmentation} \end{turn}} &
			
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_segs__78} &
			\includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_segs__zeroPadding_78} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_segs__CS_78} & \includegraphics[width=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_segs__IMCNNL2TUNE_78} & \includegraphics[width=2.5cm,height=2.5cm]{include/grp2/factor2/019-Guys-0702-T1/019-Guys-0702-T1_segs__predict_78}
			
			
		\end{tabular}
		\par\end{raggedleft}
	\raggedright{}\caption{\textcolor{black}{\footnotesize{}Examples of reconstructed MR images from under-sampled k-space - Factor 2.5.}}
	\label{fig:example_factor_2.5} 
\end{figure}



%-----------------------------BIBLIOGRAPHY--------------------%
\bibliography{paper}


\end{document}